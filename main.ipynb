{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5445fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d02c5",
   "metadata": {},
   "source": [
    "### VGG16 Model\n",
    "\n",
    "Detailed description of the architecture.\n",
    "\n",
    "1. **First block:**\n",
    "   - 1.1 Convolutional layer: 3 input channels (RGB), 64 output channels, 3×3 kernel size, stride=1, and padding of 1 pixel (to maintain spatial dimensions)\n",
    "   - 1.2 Convolutional layer: 64 input channels (RGB), 64 output channels, 3×3 kernel size, stride=1, and padding of 1 pixel (to maintain spatial dimensions)\n",
    "   - 1.3 MaxPool layer: kernel size of 2x2, stride=2\n",
    "\n",
    "2. **Second Block:**\n",
    "   - 2.1 Convolutional layer: 64 input channels, 128 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 2.2 Convolutional layer: 128 input channels, 128 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 2.3 MaxPool layer: kernel size of 2x2, stride=2\n",
    "\n",
    "3. **Third Block:**\n",
    "   - 3.1 Convolutional layer: 128 input channels, 256 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 3.2 Convolutional layer: 256 input channels, 256 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 3.3 Convolutional layer: 256 input channels, 256 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 3.4 MaxPool layer: kernel size of 2x2, stride=2\n",
    "\n",
    "4. **Fourth Block:**\n",
    "   - 4.1 Convolutional layer: 256 input channels, 512 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 4.2 Convolutional layer: 512 input channels, 512 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 4.3 Convolutional layer: 512 input channels, 512 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 4.4 MaxPool layer: kernel size of 2x2, stride=2\n",
    "\n",
    "5. **Fifth Block:**\n",
    "   - 5.1 Convolutional layer: 256 input channels, 512 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 5.2 Convolutional layer: 512 input channels, 512 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 5.3 Convolutional layer: 512 input channels, 512 output channels, 3x3 kernel size, stride=1, and padding of 1 pixel\n",
    "   - 5.4 MaxPool layer: kernel size of 2x2, stride=2\n",
    "\n",
    "6. **Sixth Block:**\n",
    "   - 6.1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0f29ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(VGG16, self).__init__()\n",
    "\n",
    "        # First block\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),  # input: 3 x 224 x 224\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # output: 64 x 112 x 112\n",
    "        )\n",
    "        \n",
    "        # Second block\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # output: 128 x 56 x 56\n",
    "        )\n",
    "\n",
    "        # Third block\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   # output: 256 x 28 x 28\n",
    "        )\n",
    "\n",
    "        # Fourth block\n",
    "        self.block4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   # output: 512 x 14 x 14\n",
    "        )\n",
    "\n",
    "        # Fifth block\n",
    "        self.block5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)   # output: 512 x 7 x 7\n",
    "        )\n",
    "\n",
    "        # Sixth block (dense layers)\n",
    "        self.block6 = nn.Sequential(\n",
    "            nn.Flatten(),   # Flatten the feature map (512 x 7 x 7) into a vector\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.block4(out)\n",
    "        out = self.block5(out)    \n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.block6(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01ab0d7",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "We will use the CIFAR10 dataset, which has 10 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dafe38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2dc962d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Re-size images for the VGG16\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),    # Normalization\n",
    "])\n",
    "\n",
    "# Split into train, test, and validations sets\n",
    "train_set = CIFAR10(\"dataset/\", train=True, transform=transform, download=True)\n",
    "val_size = math.ceil(len(train_set) * 0.1)  # 10% of the dataset\n",
    "train_size = len(train_set) - val_size\n",
    "train_set, val_set = random_split(train_set, [train_size, val_size])  # Split into train and validation sets\n",
    "test_set = CIFAR10(\"dataset/\", train=False, transform=transform, download=True)\n",
    "\n",
    "# Use dataloaders\n",
    "train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, BATCH_SIZE, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00415e8",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e367096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "vgg16_model = VGG16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0431d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "NUM_EPOCH = 5\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "# Set loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(vgg16_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = vgg16_model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()   # clear previous gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Step:{i+1}, Loss: {loss.item()}\")\n",
    "\n",
    "    # Validations\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in val_loader:\n",
    "            outputs = vgg16_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            del images, labels, outputs\n",
    "    \n",
    "        print('Accuracy of the network on the {} validation images: {} %'.format(5000, 100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
